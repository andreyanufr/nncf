import torch
import numpy as np

q_vectors_256 = torch.tensor(
    [
        [8, 8, 8, 8, 8, 8, 8, 8],
        [43, 8, 8, 8, 8, 8, 8, 8],
        [25, 25, 8, 8, 8, 8, 8, 8],
        [8, 43, 8, 8, 8, 8, 8, 8],
        [43, 43, 8, 8, 8, 8, 8, 8],
        [25, 8, 25, 8, 8, 8, 8, 8],
        [8, 25, 25, 8, 8, 8, 8, 8],
        [8, 8, 43, 8, 8, 8, 8, 8],
        [43, 8, 43, 8, 8, 8, 8, 8],
        [8, 43, 43, 8, 8, 8, 8, 8],
        [43, 43, 43, 8, 8, 8, 8, 8],
        [25, 8, 8, 25, 8, 8, 8, 8],
        [8, 25, 8, 25, 8, 8, 8, 8],
        [8, 8, 25, 25, 8, 8, 8, 8],
        [8, 43, 25, 25, 8, 8, 8, 8],
        [25, 8, 43, 25, 8, 8, 8, 8],
        [8, 25, 43, 25, 8, 8, 8, 8],
        [8, 8, 8, 43, 8, 8, 8, 8],
        [43, 8, 8, 43, 8, 8, 8, 8],
        [43, 43, 8, 43, 8, 8, 8, 8],
        [43, 8, 43, 43, 8, 8, 8, 8],
        [25, 8, 8, 8, 25, 8, 8, 8],
        [8, 25, 8, 8, 25, 8, 8, 8],
        [8, 8, 25, 8, 25, 8, 8, 8],
        [25, 25, 25, 8, 25, 8, 8, 8],
        [8, 8, 8, 25, 25, 8, 8, 8],
        [8, 25, 8, 43, 25, 8, 8, 8],
        [8, 43, 25, 43, 25, 8, 8, 8],
        [8, 8, 8, 8, 43, 8, 8, 8],
        [43, 8, 8, 8, 43, 8, 8, 8],
        [43, 8, 43, 8, 43, 8, 8, 8],
        [43, 8, 8, 43, 43, 8, 8, 8],
        [25, 8, 8, 8, 8, 25, 8, 8],
        [8, 25, 8, 8, 8, 25, 8, 8],
        [8, 8, 25, 8, 8, 25, 8, 8],
        [25, 8, 43, 8, 8, 25, 8, 8],
        [8, 25, 43, 8, 8, 25, 8, 8],
        [8, 8, 8, 25, 8, 25, 8, 8],
        [43, 8, 8, 25, 8, 25, 8, 8],
        [8, 43, 8, 25, 8, 25, 8, 8],
        [8, 8, 43, 25, 8, 25, 8, 8],
        [25, 8, 8, 43, 8, 25, 8, 8],
        [8, 25, 8, 43, 8, 25, 8, 8],
        [8, 8, 25, 43, 8, 25, 8, 8],
        [8, 25, 43, 43, 8, 25, 8, 8],
        [8, 8, 8, 8, 25, 25, 8, 8],
        [43, 8, 8, 8, 25, 25, 8, 8],
        [8, 43, 8, 8, 25, 25, 8, 8],
        [8, 8, 43, 8, 25, 25, 8, 8],
        [43, 25, 8, 25, 25, 25, 8, 8],
        [25, 43, 43, 25, 25, 25, 8, 8],
        [8, 8, 8, 43, 25, 25, 8, 8],
        [25, 8, 25, 43, 25, 25, 8, 8],
        [25, 43, 8, 8, 43, 25, 8, 8],
        [8, 8, 25, 8, 43, 25, 8, 8],
        [8, 8, 8, 25, 43, 25, 8, 8],
        [8, 25, 8, 43, 43, 25, 8, 8],
        [8, 25, 43, 43, 43, 25, 8, 8],
        [8, 8, 8, 8, 8, 43, 8, 8],
        [25, 25, 8, 8, 8, 43, 8, 8],
        [8, 43, 8, 8, 8, 43, 8, 8],
        [8, 25, 25, 8, 8, 43, 8, 8],
        [8, 43, 43, 8, 8, 43, 8, 8],
        [25, 8, 8, 25, 8, 43, 8, 8],
        [8, 25, 8, 25, 8, 43, 8, 8],
        [8, 8, 25, 25, 8, 43, 8, 8],
        [43, 8, 25, 25, 8, 43, 8, 8],
        [8, 43, 8, 43, 8, 43, 8, 8],
        [8, 25, 8, 8, 25, 43, 8, 8],
        [8, 8, 8, 25, 25, 43, 8, 8],
        [43, 8, 8, 8, 43, 43, 8, 8],
        [8, 25, 25, 8, 43, 43, 8, 8],
        [25, 8, 8, 8, 8, 8, 25, 8],
        [8, 25, 8, 8, 8, 8, 25, 8],
        [8, 8, 25, 8, 8, 8, 25, 8],
        [25, 8, 43, 8, 8, 8, 25, 8],
        [8, 8, 8, 25, 8, 8, 25, 8],
        [8, 8, 43, 25, 8, 8, 25, 8],
        [8, 25, 8, 43, 8, 8, 25, 8],
        [8, 8, 25, 43, 8, 8, 25, 8],
        [25, 25, 25, 43, 8, 8, 25, 8],
        [8, 8, 8, 8, 25, 8, 25, 8],
        [8, 43, 8, 8, 25, 8, 25, 8],
        [8, 8, 43, 8, 25, 8, 25, 8],
        [8, 8, 25, 25, 25, 8, 25, 8],
        [43, 43, 25, 25, 25, 8, 25, 8],
        [8, 8, 8, 43, 25, 8, 25, 8],
        [8, 25, 43, 8, 43, 8, 25, 8],
        [25, 25, 8, 25, 43, 8, 25, 8],
        [8, 8, 8, 8, 8, 25, 25, 8],
        [8, 43, 8, 8, 8, 25, 25, 8],
        [8, 8, 43, 8, 8, 25, 25, 8],
        [25, 25, 43, 8, 8, 25, 25, 8],
        [25, 43, 8, 25, 8, 25, 25, 8],
        [8, 8, 8, 43, 8, 25, 25, 8],
        [8, 43, 25, 8, 25, 25, 25, 8],
        [43, 8, 43, 25, 25, 25, 25, 8],
        [8, 8, 8, 8, 43, 25, 25, 8],
        [43, 25, 25, 8, 43, 25, 25, 8],
        [25, 8, 8, 8, 8, 43, 25, 8],
        [8, 25, 8, 8, 8, 43, 25, 8],
        [8, 8, 25, 8, 8, 43, 25, 8],
        [8, 8, 8, 25, 8, 43, 25, 8],
        [25, 8, 8, 43, 8, 43, 25, 8],
        [8, 8, 8, 8, 25, 43, 25, 8],
        [25, 25, 8, 8, 25, 43, 25, 8],
        [8, 8, 43, 43, 25, 43, 25, 8],
        [25, 8, 25, 25, 43, 43, 25, 8],
        [8, 8, 8, 8, 8, 8, 43, 8],
        [43, 8, 8, 8, 8, 8, 43, 8],
        [43, 43, 8, 8, 8, 8, 43, 8],
        [8, 25, 8, 25, 8, 8, 43, 8],
        [25, 8, 43, 25, 8, 8, 43, 8],
        [8, 8, 8, 43, 8, 8, 43, 8],
        [43, 8, 8, 43, 8, 8, 43, 8],
        [25, 43, 43, 8, 25, 8, 43, 8],
        [8, 43, 8, 25, 25, 8, 43, 8],
        [8, 8, 8, 8, 43, 8, 43, 8],
        [43, 8, 8, 8, 43, 8, 43, 8],
        [25, 8, 8, 8, 8, 25, 43, 8],
        [8, 25, 8, 8, 8, 25, 43, 8],
        [8, 8, 25, 8, 8, 25, 43, 8],
        [8, 8, 8, 25, 8, 25, 43, 8],
        [43, 25, 25, 25, 8, 25, 43, 8],
        [8, 8, 8, 8, 25, 25, 43, 8],
        [25, 8, 8, 25, 25, 25, 43, 8],
        [8, 25, 43, 25, 25, 25, 43, 8],
        [8, 8, 25, 43, 43, 25, 43, 8],
        [8, 43, 8, 8, 8, 43, 43, 8],
        [8, 8, 43, 8, 8, 43, 43, 8],
        [8, 25, 25, 43, 8, 43, 43, 8],
        [8, 25, 8, 25, 43, 43, 43, 8],
        [25, 8, 8, 8, 8, 8, 8, 25],
        [8, 25, 8, 8, 8, 8, 8, 25],
        [8, 8, 25, 8, 8, 8, 8, 25],
        [8, 43, 25, 8, 8, 8, 8, 25],
        [25, 8, 43, 8, 8, 8, 8, 25],
        [8, 25, 43, 8, 8, 8, 8, 25],
        [8, 8, 8, 25, 8, 8, 8, 25],
        [8, 43, 8, 25, 8, 8, 8, 25],
        [43, 25, 25, 25, 8, 8, 8, 25],
        [8, 8, 43, 25, 8, 8, 8, 25],
        [25, 8, 8, 43, 8, 8, 8, 25],
        [8, 25, 8, 43, 8, 8, 8, 25],
        [8, 8, 25, 43, 8, 8, 8, 25],
        [8, 8, 8, 8, 25, 8, 8, 25],
        [8, 8, 43, 8, 25, 8, 8, 25],
        [25, 8, 43, 25, 25, 8, 8, 25],
        [8, 8, 8, 43, 25, 8, 8, 25],
        [25, 25, 8, 43, 25, 8, 8, 25],
        [25, 8, 8, 8, 43, 8, 8, 25],
        [8, 8, 25, 8, 43, 8, 8, 25],
        [8, 43, 8, 25, 43, 8, 8, 25],
        [43, 25, 25, 25, 43, 8, 8, 25],
        [8, 43, 43, 25, 43, 8, 8, 25],
        [8, 8, 8, 8, 8, 25, 8, 25],
        [8, 43, 8, 8, 8, 25, 8, 25],
        [8, 8, 43, 8, 8, 25, 8, 25],
        [8, 8, 8, 43, 8, 25, 8, 25],
        [25, 43, 25, 43, 8, 25, 8, 25],
        [43, 8, 25, 8, 25, 25, 8, 25],
        [8, 25, 43, 8, 25, 25, 8, 25],
        [8, 8, 8, 8, 43, 25, 8, 25],
        [25, 8, 8, 8, 8, 43, 8, 25],
        [8, 25, 8, 8, 8, 43, 8, 25],
        [8, 8, 25, 8, 8, 43, 8, 25],
        [8, 8, 8, 25, 8, 43, 8, 25],
        [25, 25, 8, 25, 8, 43, 8, 25],
        [8, 8, 8, 8, 25, 43, 8, 25],
        [8, 43, 25, 25, 25, 43, 8, 25],
        [25, 8, 43, 25, 25, 43, 8, 25],
        [43, 8, 8, 43, 25, 43, 8, 25],
        [25, 25, 8, 25, 43, 43, 8, 25],
        [8, 8, 25, 43, 43, 43, 8, 25],
        [8, 8, 8, 8, 8, 8, 25, 25],
        [8, 43, 8, 8, 8, 8, 25, 25],
        [25, 8, 25, 8, 8, 8, 25, 25],
        [25, 43, 25, 8, 8, 8, 25, 25],
        [8, 8, 43, 8, 8, 8, 25, 25],
        [8, 8, 8, 43, 8, 8, 25, 25],
        [8, 43, 8, 43, 8, 8, 25, 25],
        [8, 25, 8, 8, 25, 8, 25, 25],
        [43, 8, 8, 25, 25, 8, 25, 25],
        [8, 25, 43, 43, 25, 8, 25, 25],
        [25, 8, 25, 43, 43, 8, 25, 25],
        [8, 8, 25, 43, 8, 25, 25, 25],
        [43, 8, 25, 43, 8, 25, 25, 25],
        [43, 43, 8, 8, 25, 25, 25, 25],
        [25, 8, 8, 8, 43, 25, 25, 25],
        [8, 25, 25, 25, 43, 25, 25, 25],
        [8, 8, 8, 8, 8, 43, 25, 25],
        [25, 8, 25, 8, 8, 43, 25, 25],
        [25, 43, 25, 8, 8, 43, 25, 25],
        [8, 25, 43, 25, 8, 43, 25, 25],
        [8, 8, 8, 25, 25, 43, 25, 25],
        [8, 43, 8, 8, 43, 43, 25, 25],
        [8, 25, 8, 8, 8, 8, 43, 25],
        [8, 8, 25, 8, 8, 8, 43, 25],
        [8, 8, 8, 25, 8, 8, 43, 25],
        [8, 43, 43, 25, 8, 8, 43, 25],
        [8, 8, 8, 8, 25, 8, 43, 25],
        [25, 25, 25, 25, 25, 8, 43, 25],
        [8, 43, 25, 8, 43, 8, 43, 25],
        [8, 8, 43, 25, 43, 8, 43, 25],
        [8, 8, 8, 8, 8, 25, 43, 25],
        [25, 25, 8, 8, 8, 25, 43, 25],
        [8, 8, 25, 8, 25, 25, 43, 25],
        [43, 8, 25, 8, 25, 25, 43, 25],
        [8, 25, 8, 43, 25, 25, 43, 25],
        [43, 8, 8, 25, 8, 43, 43, 25],
        [8, 8, 8, 8, 8, 8, 8, 43],
        [43, 8, 8, 8, 8, 8, 8, 43],
        [43, 43, 8, 8, 8, 8, 8, 43],
        [25, 8, 8, 25, 8, 8, 8, 43],
        [43, 8, 8, 43, 8, 8, 8, 43],
        [8, 25, 8, 8, 25, 8, 8, 43],
        [8, 43, 25, 8, 25, 8, 8, 43],
        [8, 8, 8, 25, 25, 8, 8, 43],
        [25, 8, 25, 8, 43, 8, 8, 43],
        [25, 8, 8, 8, 8, 25, 8, 43],
        [8, 25, 8, 8, 8, 25, 8, 43],
        [8, 8, 25, 8, 8, 25, 8, 43],
        [25, 25, 25, 8, 8, 25, 8, 43],
        [8, 8, 8, 25, 8, 25, 8, 43],
        [8, 8, 43, 25, 8, 25, 8, 43],
        [8, 8, 8, 8, 25, 25, 8, 43],
        [43, 25, 8, 25, 25, 25, 8, 43],
        [8, 25, 25, 43, 25, 25, 8, 43],
        [25, 43, 8, 8, 43, 25, 8, 43],
        [8, 8, 8, 25, 43, 25, 8, 43],
        [8, 8, 43, 25, 43, 25, 8, 43],
        [43, 8, 8, 8, 8, 43, 8, 43],
        [8, 25, 8, 8, 25, 43, 8, 43],
        [25, 8, 25, 8, 43, 43, 8, 43],
        [8, 25, 8, 8, 8, 8, 25, 43],
        [8, 8, 25, 8, 8, 8, 25, 43],
        [8, 25, 43, 8, 8, 8, 25, 43],
        [8, 8, 8, 25, 8, 8, 25, 43],
        [25, 8, 43, 43, 8, 8, 25, 43],
        [43, 25, 25, 8, 25, 8, 25, 43],
        [8, 8, 8, 43, 25, 8, 25, 43],
        [25, 25, 8, 25, 43, 8, 25, 43],
        [8, 8, 8, 8, 8, 25, 25, 43],
        [43, 8, 43, 8, 8, 25, 25, 43],
        [8, 25, 8, 25, 8, 25, 25, 43],
        [25, 8, 25, 25, 25, 25, 25, 43],
        [25, 8, 8, 43, 8, 43, 25, 43],
        [8, 8, 43, 8, 25, 43, 25, 43],
        [43, 8, 8, 8, 8, 8, 43, 43],
        [8, 8, 25, 25, 8, 8, 43, 43],
        [25, 25, 8, 43, 8, 8, 43, 43],
        [25, 43, 8, 8, 25, 8, 43, 43],
        [8, 8, 8, 8, 43, 8, 43, 43],
        [8, 43, 25, 8, 8, 25, 43, 43],
        [8, 8, 25, 25, 8, 43, 43, 43],
        [8, 25, 8, 8, 25, 43, 43, 43],
    ]
)


def get_packed_abs_grid_4096(scale=127):
    intr = torch.arange(-4, 4)
    d8 = torch.cartesian_prod(*[intr] * 8).float() + 1 / 2
    d8m2 = (d8.sum(dim=-1) % 2 == 0)
    d8n = d8.norm(dim=-1)**2 <= 22
    d8abs = torch.unique(d8[sorted(torch.where(d8m2 * d8n)[0])].abs(), dim=0)
    cba = d8abs[:4096, :]
    cba = cba / cba.max() * scale
    cba = cba.long()
    # for i in range(4096):
    #     print(cba[i, :])
    return cba

def get_packed_abs_grid_d4():
    intr = torch.arange(-8, 8)
    d4 = torch.cartesian_prod(*[intr] * 4).float() + 1/ 2
    d4m2 = (d4.sum(dim=-1) % 2 == 0)

    d4abs = torch.unique(d4[sorted(torch.where(d4m2)[0])].abs(), dim=0)

    return d4abs

def get_initial_array():
    name = "/home/aanuf/libs/nncf_aa/nncf/experimental/torch/vector_quantization/cba.npy"
    res = np.load(name)
    res = torch.from_numpy(res)
    return res
