import torch.nn as nn

llama_3_8b_instruct_scores = {
    "layers.0.self_attn.k_proj" : 1825286.0164468547,
    "layers.0.self_attn.v_proj" : 30279689.0695911,
    "layers.0.self_attn.q_proj" : 1845129.3204905626,
    "layers.0.self_attn.o_proj" : 4670.938367343242,
    "layers.0.mlp.up_proj" : 933755.3814855301,
    "layers.0.mlp.gate_proj" : 168980.10471032877,
    "layers.0.mlp.down_proj" : 286670.85367271357,
    "layers.1.self_attn.k_proj" : 2857433.722057082,
    "layers.1.self_attn.v_proj" : 10772025.085986335,
    "layers.1.self_attn.q_proj" : 2620378.896858815,
    "layers.1.self_attn.o_proj" : 6702.203866136726,
    "layers.1.mlp.up_proj" : 263722.1690204692,
    "layers.1.mlp.gate_proj" : 205961.20845969612,
    "layers.1.mlp.down_proj" : 1132799962.0437188,
    "layers.2.self_attn.k_proj" : 7822518.163159242,
    "layers.2.self_attn.v_proj" : 32460168.80389188,
    "layers.2.self_attn.q_proj" : 10428143.93081033,
    "layers.2.self_attn.o_proj" : 34950.11558754123,
    "layers.2.mlp.up_proj" : 1354957.827854864,
    "layers.2.mlp.gate_proj" : 757573.3176709343,
    "layers.2.mlp.down_proj" : 32588.675411146447,
    "layers.3.self_attn.k_proj" : 7171592.250835498,
    "layers.3.self_attn.v_proj" : 17415520.21590486,
    "layers.3.self_attn.q_proj" : 8498240.673514217,
    "layers.3.self_attn.o_proj" : 40072.18696851448,
    "layers.3.mlp.up_proj" : 1799462.1211905342,
    "layers.3.mlp.gate_proj" : 625596.7790223524,
    "layers.3.mlp.down_proj" : 53339.12896689328,
    "layers.4.self_attn.k_proj" : 5105747.7236252,
    "layers.4.self_attn.v_proj" : 19727889.192383513,
    "layers.4.self_attn.q_proj" : 5877431.724336452,
    "layers.4.self_attn.o_proj" : 90561.20999896475,
    "layers.4.mlp.up_proj" : 2076822.612068733,
    "layers.4.mlp.gate_proj" : 1392269.9905460419,
    "layers.4.mlp.down_proj" : 77671.18314127471,
    "layers.5.self_attn.k_proj" : 3279561.3286310052,
    "layers.5.self_attn.v_proj" : 20087748.388569638,
    "layers.5.self_attn.q_proj" : 11025209.651435612,
    "layers.5.self_attn.o_proj" : 118048.33400187618,
    "layers.5.mlp.up_proj" : 1827001.6754165185,
    "layers.5.mlp.gate_proj" : 1334772.6594048673,
    "layers.5.mlp.down_proj" : 94837.0629432373,
    "layers.6.self_attn.k_proj" : 11676670.312631026,
    "layers.6.self_attn.v_proj" : 35605923.025445275,
    "layers.6.self_attn.q_proj" : 15060188.959832493,
    "layers.6.self_attn.o_proj" : 50826.090996058854,
    "layers.6.mlp.up_proj" : 4631130.935908572,
    "layers.6.mlp.gate_proj" : 1489352.0620077534,
    "layers.6.mlp.down_proj" : 79310.87198004003,
    "layers.7.self_attn.k_proj" : 10905584.653927147,
    "layers.7.self_attn.v_proj" : 29393427.045573443,
    "layers.7.self_attn.q_proj" : 13228159.485089827,
    "layers.7.self_attn.o_proj" : 204246.28215611645,
    "layers.7.mlp.up_proj" : 3760405.1730250544,
    "layers.7.mlp.gate_proj" : 1885463.3656861146,
    "layers.7.mlp.down_proj" : 61420.56212502272,
    "layers.8.self_attn.k_proj" : 6256558.5426903535,
    "layers.8.self_attn.v_proj" : 37728041.34540815,
    "layers.8.self_attn.q_proj" : 6096857.526564829,
    "layers.8.self_attn.o_proj" : 183148.31565640814,
    "layers.8.mlp.up_proj" : 2944709.9591066083,
    "layers.8.mlp.gate_proj" : 2102601.217565129,
    "layers.8.mlp.down_proj" : 60854.232606466656,
    "layers.9.self_attn.v_proj" : 47002559.1050826,
    "layers.9.self_attn.k_proj" : 16185907.592058007,
    "layers.9.self_attn.q_proj" : 20456715.617807433,
    "layers.9.self_attn.o_proj" : 302840.9254018592,
    "layers.9.mlp.up_proj" : 4876200.539719269,
    "layers.9.mlp.gate_proj" : 1619220.3814113126,
    "layers.9.mlp.down_proj" : 64556.0465890655,
    "layers.10.self_attn.k_proj" : 10058991.201542899,
    "layers.10.self_attn.v_proj" : 64237761.53119498,
    "layers.10.self_attn.q_proj" : 12495025.277928228,
    "layers.10.self_attn.o_proj" : 192775.68836450847,
    "layers.10.mlp.up_proj" : 3943675.5143083604,
    "layers.10.mlp.gate_proj" : 2063234.1073520046,
    "layers.10.mlp.down_proj" : 197305.86887816,
    "layers.11.self_attn.k_proj" : 21519422.463196084,
    "layers.11.self_attn.v_proj" : 70830129.93760109,
    "layers.11.self_attn.q_proj" : 37344281.106477596,
    "layers.11.self_attn.o_proj" : 420795.52386845404,
    "layers.11.mlp.up_proj" : 5462125.128891707,
    "layers.11.mlp.gate_proj" : 1912385.659339211,
    "layers.11.mlp.down_proj" : 47521.91199028797,
    "layers.12.self_attn.k_proj" : 23234842.379158873,
    "layers.12.self_attn.v_proj" : 47545976.03836576,
    "layers.12.self_attn.q_proj" : 22495137.839628793,
    "layers.12.self_attn.o_proj" : 199469.35049549586,
    "layers.12.mlp.up_proj" : 2981355.0331351305,
    "layers.12.mlp.gate_proj" : 1775584.1834985125,
    "layers.12.mlp.down_proj" : 25464.424222989695,
    "layers.13.self_attn.k_proj" : 23062765.610452868,
    "layers.13.self_attn.v_proj" : 84432667.15030588,
    "layers.13.self_attn.q_proj" : 47481591.38456956,
    "layers.13.self_attn.o_proj" : 96687.08744261994,
    "layers.13.mlp.up_proj" : 3521452.1931159515,
    "layers.13.mlp.gate_proj" : 1774046.4484095264,
    "layers.13.mlp.down_proj" : 47879.38283650918,
    "layers.14.self_attn.k_proj" : 16407923.72158774,
    "layers.14.self_attn.v_proj" : 85162442.11374716,
    "layers.14.self_attn.q_proj" : 20455975.867949974,
    "layers.14.self_attn.o_proj" : 637446.9113369456,
    "layers.14.mlp.up_proj" : 2929375.495178727,
    "layers.14.mlp.gate_proj" : 1635277.5503566745,
    "layers.14.mlp.down_proj" : 164450.34012507708,
    "layers.15.self_attn.k_proj" : 21985858.90125694,
    "layers.15.self_attn.v_proj" : 67125921.34266523,
    "layers.15.self_attn.q_proj" : 14401164.78367213,
    "layers.15.self_attn.o_proj" : 184194.06732784057,
    "layers.15.mlp.up_proj" : 5366962.773180354,
    "layers.15.mlp.gate_proj" : 1947211.961691378,
    "layers.15.mlp.down_proj" : 234913.7245235625,
    "layers.16.self_attn.k_proj" : 20497653.36452128,
    "layers.16.self_attn.v_proj" : 70636015.12588376,
    "layers.16.self_attn.q_proj" : 13697810.233880794,
    "layers.16.self_attn.o_proj" : 165392.55961838295,
    "layers.16.mlp.up_proj" : 5245452.64076648,
    "layers.16.mlp.gate_proj" : 2231190.7259334945,
    "layers.16.mlp.down_proj" : 241266.00965341873,
    "layers.17.self_attn.k_proj" : 15338740.68017331,
    "layers.17.self_attn.v_proj" : 64966016.85078128,
    "layers.17.self_attn.q_proj" : 30065835.204322334,
    "layers.17.self_attn.o_proj" : 103335.15849767275,
    "layers.17.mlp.up_proj" : 3253807.458339113,
    "layers.17.mlp.gate_proj" : 2289872.113154371,
    "layers.17.mlp.down_proj" : 490253.2798043025,
    "layers.18.self_attn.k_proj" : 19541206.354310136,
    "layers.18.self_attn.v_proj" : 63827109.62448112,
    "layers.18.self_attn.q_proj" : 29255117.998988878,
    "layers.18.self_attn.o_proj" : 95587.80833229289,
    "layers.18.mlp.up_proj" : 4166030.1241617682,
    "layers.18.mlp.gate_proj" : 1710413.7804165652,
    "layers.18.mlp.down_proj" : 476841.21799747605,
    "layers.19.self_attn.k_proj" : 18384113.3244741,
    "layers.19.self_attn.v_proj" : 53668073.230260715,
    "layers.19.self_attn.q_proj" : 24343336.279538527,
    "layers.19.self_attn.o_proj" : 86597.22104866728,
    "layers.19.mlp.up_proj" : 3627578.512481903,
    "layers.19.mlp.gate_proj" : 1387092.823810983,
    "layers.19.mlp.down_proj" : 975275.7513824202,
    "layers.20.self_attn.k_proj" : 19735095.365961164,
    "layers.20.self_attn.v_proj" : 63305131.59495402,
    "layers.20.self_attn.q_proj" : 23839348.63911144,
    "layers.20.self_attn.o_proj" : 56375.35487184526,
    "layers.20.mlp.up_proj" : 3878603.0770710176,
    "layers.20.mlp.gate_proj" : 2085382.9664912727,
    "layers.20.mlp.down_proj" : 952136.6381700697,
    "layers.21.self_attn.k_proj" : 15828782.866106244,
    "layers.21.self_attn.v_proj" : 48362336.56630729,
    "layers.21.self_attn.q_proj" : 21533732.71624029,
    "layers.21.self_attn.o_proj" : 752421.8217422307,
    "layers.21.mlp.up_proj" : 3031997.860209216,
    "layers.21.mlp.gate_proj" : 1188632.3038225542,
    "layers.21.mlp.down_proj" : 468699.8514321,
    "layers.22.self_attn.k_proj" : 15195111.114672296,
    "layers.22.self_attn.v_proj" : 63549254.92589762,
    "layers.22.self_attn.q_proj" : 28939785.910466503,
    "layers.22.self_attn.o_proj" : 222638.43488310452,
    "layers.22.mlp.up_proj" : 6278043.352108268,
    "layers.22.mlp.gate_proj" : 2143839.9679477876,
    "layers.22.mlp.down_proj" : 753655.455955756,
    "layers.23.self_attn.k_proj" : 12202691.591052951,
    "layers.23.self_attn.v_proj" : 49751935.84398336,
    "layers.23.self_attn.q_proj" : 19812597.96359589,
    "layers.23.self_attn.o_proj" : 330382.8397514139,
    "layers.23.mlp.up_proj" : 5766164.179761471,
    "layers.23.mlp.gate_proj" : 2304873.632834275,
    "layers.23.mlp.down_proj" : 797642.4216446844,
    "layers.24.self_attn.k_proj" : 12769599.353858782,
    "layers.24.self_attn.v_proj" : 47243068.67388119,
    "layers.24.self_attn.q_proj" : 21152471.69945677,
    "layers.24.self_attn.o_proj" : 770604.2127412657,
    "layers.24.mlp.up_proj" : 2662684.4777052184,
    "layers.24.mlp.gate_proj" : 2117574.0512508224,
    "layers.24.mlp.down_proj" : 1049306.7440462469,
    "layers.25.self_attn.k_proj" : 10768725.463937936,
    "layers.25.self_attn.v_proj" : 40205925.1988953,
    "layers.25.self_attn.q_proj" : 17842285.411344957,
    "layers.25.self_attn.o_proj" : 403592.76313464565,
    "layers.25.mlp.up_proj" : 4964429.817912528,
    "layers.25.mlp.gate_proj" : 2838073.0514333406,
    "layers.25.mlp.down_proj" : 1289915.3186770305,
    "layers.26.self_attn.k_proj" : 12898888.664612118,
    "layers.26.self_attn.v_proj" : 32924546.28511237,
    "layers.26.self_attn.q_proj" : 21512783.01578479,
    "layers.26.self_attn.o_proj" : 474937.09101492097,
    "layers.26.mlp.up_proj" : 3351703.8884619134,
    "layers.26.mlp.gate_proj" : 2489064.493290335,
    "layers.26.mlp.down_proj" : 1023887.8332500425,
    "layers.27.self_attn.k_proj" : 10207066.64965704,
    "layers.27.self_attn.v_proj" : 23707920.63404922,
    "layers.27.self_attn.q_proj" : 18034623.518000383,
    "layers.27.self_attn.o_proj" : 438897.9982441986,
    "layers.27.mlp.up_proj" : 6030795.212872516,
    "layers.27.mlp.gate_proj" : 2514867.9804686178,
    "layers.27.mlp.down_proj" : 2340545.105218104,
    "layers.28.self_attn.k_proj" : 13537589.797755785,
    "layers.28.self_attn.v_proj" : 34232035.49247575,
    "layers.28.self_attn.q_proj" : 16013205.830172181,
    "layers.28.self_attn.o_proj" : 173861.92329325317,
    "layers.28.mlp.up_proj" : 3531418.667913468,
    "layers.28.mlp.gate_proj" : 2230937.243534732,
    "layers.28.mlp.down_proj" : 1145716.8167639945,
    "layers.29.self_attn.k_proj" : 8648045.45846923,
    "layers.29.self_attn.v_proj" : 19964139.18683251,
    "layers.29.self_attn.q_proj" : 12244769.518048622,
    "layers.29.self_attn.o_proj" : 1004576.9748346003,
    "layers.29.mlp.up_proj" : 4509892.363478211,
    "layers.29.mlp.gate_proj" : 1698416.1898727873,
    "layers.29.mlp.down_proj" : 1482002.6488152354,
    "layers.30.self_attn.k_proj" : 11140927.941010421,
    "layers.30.self_attn.v_proj" : 25726144.961997554,
    "layers.30.self_attn.q_proj" : 20134748.39635092,
    "layers.30.self_attn.o_proj" : 397110.60396504175,
    "layers.30.mlp.up_proj" : 3821168.6856185426,
    "layers.30.mlp.gate_proj" : 1228994.7126190225,
    "layers.30.mlp.down_proj" : 5798936.296168872,
    "layers.31.self_attn.v_proj" : 10425456.887935922,
    "layers.31.self_attn.k_proj" : 3177067.806926278,
    "layers.31.self_attn.q_proj" : 4053490.160714957,
    "layers.31.self_attn.o_proj" : 6223367.705202787,
    "layers.31.mlp.up_proj" : 1610107.664228992,
    "layers.31.mlp.gate_proj" : 1168278.7855071733,
    "layers.31.mlp.down_proj" : 165459711.79825217,
}

llama_3_8b_instruct_8bit_layers = [
    "layers.9.mlp.up_proj",
    "layers.25.mlp.up_proj",
    "layers.4.self_attn.k_proj",
    "layers.16.mlp.up_proj",
    "layers.15.mlp.up_proj",
    "layers.11.mlp.up_proj",
    "layers.23.mlp.up_proj",
    "layers.30.mlp.down_proj",
    "layers.4.self_attn.q_proj",
    "layers.27.mlp.up_proj",
    "layers.8.self_attn.q_proj",
    "layers.31.self_attn.o_proj",
    "layers.8.self_attn.k_proj",
    "layers.22.mlp.up_proj",
    "layers.3.self_attn.k_proj",
    "layers.2.self_attn.k_proj",
    "layers.3.self_attn.q_proj",
    "layers.29.self_attn.k_proj",
    "layers.10.self_attn.k_proj",
    "layers.27.self_attn.k_proj",
    "layers.31.self_attn.v_proj",
    "layers.2.self_attn.q_proj",
    "layers.25.self_attn.k_proj",
    "layers.1.self_attn.v_proj",
    "layers.7.self_attn.k_proj",
    "layers.5.self_attn.q_proj",
    "layers.30.self_attn.k_proj",
    "layers.6.self_attn.k_proj",
    "layers.23.self_attn.k_proj",
    "layers.29.self_attn.q_proj",
    "layers.10.self_attn.q_proj",
    "layers.24.self_attn.k_proj",
    "layers.26.self_attn.k_proj",
    "layers.7.self_attn.q_proj",
    "layers.28.self_attn.k_proj",
    "layers.16.self_attn.q_proj",
    "layers.15.self_attn.q_proj",
    "layers.6.self_attn.q_proj",
    "layers.22.self_attn.k_proj",
    "layers.17.self_attn.k_proj",
    "layers.21.self_attn.k_proj",
    "layers.28.self_attn.q_proj",
    "layers.9.self_attn.k_proj",
    "layers.14.self_attn.k_proj",
    "layers.3.self_attn.v_proj",
    "layers.25.self_attn.q_proj",
    "layers.27.self_attn.q_proj",
    "layers.19.self_attn.k_proj",
    "layers.18.self_attn.k_proj",
    "layers.4.self_attn.v_proj",
    "layers.20.self_attn.k_proj",
    "layers.23.self_attn.q_proj",
    "layers.29.self_attn.v_proj",
    "layers.5.self_attn.v_proj",
    "layers.30.self_attn.q_proj",
    "layers.14.self_attn.q_proj",
    "layers.9.self_attn.q_proj",
    "layers.16.self_attn.k_proj",
    "layers.24.self_attn.q_proj",
    "layers.26.self_attn.q_proj",
    "layers.11.self_attn.k_proj",
    "layers.21.self_attn.q_proj",
    "layers.15.self_attn.k_proj",
    "layers.12.self_attn.q_proj",
    "layers.13.self_attn.k_proj",
    "layers.12.self_attn.k_proj",
    "layers.27.self_attn.v_proj",
    "layers.20.self_attn.q_proj",
    "layers.19.self_attn.q_proj",
    "layers.30.self_attn.v_proj",
    "layers.22.self_attn.q_proj",
    "layers.18.self_attn.q_proj",
    "layers.7.self_attn.v_proj",
    "layers.17.self_attn.q_proj",
    "layers.0.self_attn.v_proj",
    "layers.2.self_attn.v_proj",
    "layers.26.self_attn.v_proj",
    "layers.28.self_attn.v_proj",
    "layers.6.self_attn.v_proj",
    "layers.11.self_attn.q_proj",
    "layers.8.self_attn.v_proj",
    "layers.25.self_attn.v_proj",
    "layers.9.self_attn.v_proj",
    "layers.24.self_attn.v_proj",
    "layers.13.self_attn.q_proj",
    "layers.12.self_attn.v_proj",
    "layers.21.self_attn.v_proj",
    "layers.23.self_attn.v_proj",
    "layers.19.self_attn.v_proj",
    "layers.20.self_attn.v_proj",
    "layers.22.self_attn.v_proj",
    "layers.18.self_attn.v_proj",
    "layers.10.self_attn.v_proj",
    "layers.17.self_attn.v_proj",
    "layers.15.self_attn.v_proj",
    "layers.16.self_attn.v_proj",
    "layers.11.self_attn.v_proj",
    "layers.13.self_attn.v_proj",
    "layers.14.self_attn.v_proj",
    "layers.31.mlp.down_proj",
    "layers.1.mlp.down_proj",
]

def get_llama_3_8b_4_2_bits(bit2_ratio=0.25):
    tmp = [[k, v] for k, v in llama_3_8b_instruct_scores.items() if not k in llama_3_8b_instruct_8bit_layers]
    tmp = sorted(tmp, key=lambda x: x[1])
    n_2_bits = int(bit2_ratio * len(llama_3_8b_instruct_scores))
    
    llama_3_8b_instruct_4bit_layers = []
    llama_3_8b_instruct_2bit_layers = []
    
    for i in range(n_2_bits):
        if 'self_attn' in tmp[i][0]:
            llama_3_8b_instruct_2bit_layers.append(tmp[i][0])
        else:
            llama_3_8b_instruct_4bit_layers.append(tmp[i][0])
    
    for i in range(n_2_bits, len(tmp)):
        llama_3_8b_instruct_4bit_layers.append(tmp[i][0])
    
    return llama_3_8b_instruct_4bit_layers, llama_3_8b_instruct_2bit_layers
    

def get_llama_3_8b_instruct_config(model, bit2_ratio=0.25):
    res = {}
    llama_3_8b_instruct_4bit_layers, llama_3_8b_instruct_2bit_layers = get_llama_3_8b_4_2_bits(bit2_ratio)
    for name, layer in model.named_modules():
        if isinstance(layer, nn.Linear):
            if name in llama_3_8b_instruct_8bit_layers:
                res[name] = 8
            elif name in llama_3_8b_instruct_4bit_layers:
                res[name] = 4
            elif name in llama_3_8b_instruct_2bit_layers:
                res[name] = 2
            else:
                assert False, "Problem for precision " + name
    print("8 bit: ", len(llama_3_8b_instruct_8bit_layers))
    print("4 bit: ", len(llama_3_8b_instruct_4bit_layers))
    print("2 bit: ", len(llama_3_8b_instruct_2bit_layers))
    print(len(llama_3_8b_instruct_8bit_layers)+len(llama_3_8b_instruct_4bit_layers)+len(llama_3_8b_instruct_2bit_layers))
    return res
